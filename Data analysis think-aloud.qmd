---
author: "Sarah Bez"
title: "Data analysis think-aloud"
format: html
editor: visual
editor_options: 
  chunk_output_type: console
---

Libraries

```{r}
library(readxl)
library(lubridate)
library(skimr)
library(tidyverse)
library(ggplot2)
library(psych)
library(DescTools)
library(lpSolve)
library(irr)
library(viridisLite)
library(viridis)
library(devtools)
library(hrbrthemes)
library(ggalt)
library(gt)
library(heuristicsmineR)
library(heuristicsmineR)
library(eventdataR)
library(bupaR)
library(petrinetR)
library(psmineR)

```

# 0. Data import: excel sheets from MAXQDA and validation due to MAXQDA issues

neu händisch erstellt von Fabian

```{r}
data_maxqda_komplett_fabian_validating <- read_excel("data/Codierte Segmente komplett_Export Fabian.xlsx")%>%
  select(Dokumentgruppe, Dokumentname, Anfang, Ende, Code)%>%
    filter(Dokumentgruppe == "Konsens")%>% # filtern, was man jeweils braucht 
  mutate(start = ymd_hms(ymd("2000-01-01") + hms(Anfang)), # Datum im Prinzip frei wählbar
                 end = ymd_hms(ymd("2000-01-01") + hms(Ende)), 
                 turnID = 1:n(),
                 code = Code)%>%
  select(-Anfang, -Ende, -Code, -turnID) # nur für Validierung beider Datensätze


```

kurz nach Ende Konsensratings Runde 1 erstellt (MAXQDA 2018, Florian)

```{r}
data_maxqda_komplett_florian_validating <- read_excel("data/MAXQDA 2018 Codings 3_fertig_Runde1.xlsx")%>%
  select(Dokumentgruppe, Dokumentname, Anfang, Ende, Code)%>%
    filter(Dokumentgruppe == "Konsens")%>% # filtern, was man jeweils braucht 
  mutate(start = ymd_hms(ymd("2000-01-01") + hms(Anfang)), # Datum im Prinzip frei wählbar
                 end = ymd_hms(ymd("2000-01-01") + hms(Ende)), 
                 turnID = 1:n(),
                 code = Code)%>%
  select(-Anfang, -Ende, -Code, -turnID) # nur für Validierung beider Datensätze 

```

Check beider df

```{r}
anti_join(data_maxqda_komplett_fabian_validating, data_maxqda_komplett_florian_validating)

anti_join(data_maxqda_komplett_florian_validating, data_maxqda_komplett_fabian_validating)

```

beide sind quasi gleich. Was bei Fabian zusätzilch noch drin ist, waren alte Kodierungen ganz am Anfang von Kategorien, die dann nicht weiterverfolgt wurden. turnID muss beachtet werden (das macht auch Sinn)

# 1. Start wrangling

## event df, bei dem Rundungsproblem noch nicht berücksichtig ist

```{r}
data_konsens_event <- read_excel("data/MAXQDA 2018 Codings 3_fertig_Runde1.xlsx")%>%
  select(Dokumentgruppe, Dokumentname, Anfang, Ende, Code)%>%
    filter(Dokumentgruppe == "Konsens")%>% # filtern, was man jeweils braucht 
  mutate(start = ymd_hms(ymd("2000-01-01") + hms(Anfang)), # Datum im Prinzip frei wählbar
                 end = ymd_hms(ymd("2000-01-01") + hms(Ende)), 
                 turnID = 1:n(),
                 code = Code)%>%
  select(-Anfang, -Ende, -Code)
```

## Wrangling Rundungsproblem timestamps mit lubridate

```{r}
data_konsens_event_corrected <- 
read_excel("data/Codierte Segmente komplett_Export Fabian.xlsx")%>%
  select(Dokumentgruppe, Dokumentname, Anfang, Ende, Code)%>%
  filter(Dokumentgruppe == "Konsens")%>%
  mutate(Anfang_2 = str_sub(Anfang, -1),
         Ende_2 = str_sub(Ende, -1))%>%
    mutate(start_1 = ymd_hms(ymd("2000-01-01") + hms(Anfang)), # Datum im Prinzip frei wählbar
                 end_1 = ymd_hms(ymd("2000-01-01") + hms(Ende)), 
                 turnID = 1:n(),
                 code = Code)%>%
  mutate(start = case_when(Anfang_2 > 4 ~ (ymd_hms(start_1) + seconds(1)), TRUE~ ymd_hms(start_1)))%>%
  mutate(end = case_when(Ende_2 > 4 ~ (ymd_hms(end_1) + seconds(1)), TRUE ~ ymd_hms(end_1)))%>%
  select(Dokumentname, Dokumentgruppe, code, turnID, start, end)
  

  
```

## events to turns

```{r}
data_timesampling <- data_konsens_event_corrected%>%
  mutate(time = map2(start, end, seq, by = "1 sec"))%>%
  unnest(cols = time)%>%
  select(-start, -end, -turnID)%>%
  group_by(time, Dokumentname, Dokumentgruppe)%>%
  summarise(code = toString(code))%>%
  ungroup()%>%
  splitstackshape::cSplit_e("code", sep = ",", type = "character", drop = F, fill = 0)%>%
  as_tibble()%>%
  arrange(Dokumentname) 


```

## Datengrundlage nur offener think-aloud timesampling

```{r}
data_timesampling_ta_o <- data_timesampling%>%
  select(Dokumentname, code_RG, code_FA, code_AB, code_HM, code_SU, code_TA_O, time)%>%
  filter(code_TA_O == 1)
```

## filtering event df nur offener Think-aloud Teil (TA_O): alles, was vor dem ende des (letzten vergebenen) TA_O event liegt

```{r}
# wenn Lehrpersonen vor dem Stimulus angefangen haben, laut zu denken, gibt es mehrere TA_O-Kodierungen. Daher filterung nach dem max von TA_O innerhalb von Personen
# 
max_TA_O <- data_konsens_event_corrected%>% 
  select(-Dokumentgruppe)%>%
  filter(code %in% c("TA_O"))%>%
  group_by(Dokumentname)%>%
  summarize(max_TA_O = max(ymd_hms(end)))

konsens_event_ta_o <- data_konsens_event_corrected%>% 
  select(-Dokumentgruppe)%>%
  filter(code %in% c("RG", "AB", "FA", "HM"))%>%
  left_join(., max_TA_O, by = "Dokumentname")%>%
  filter(end <= max_TA_O)

```

# 2. Check überlappende Kodierungen

## Grafikcodes

dürfte nicht so stark sein. Kategorien: \* AGG: Aufgabengrafik \* KLG: Klassengrafik \* VLG: Verlaufsgrafik \* KG: Kompetenzgrafik \* TG: Themengrafik

Überlappungen: AGG - KLG AGG - VLG AGG - KG AGG - TG KLG - VLG KLG - KG KLG - TG VLG - KG VLG - TG KG - TG

```{r}
data_timesampling%>%
  select(Dokumentname, code_AGG, code_KLG, code_VLG, code_KG, code_TG, code_SU)%>%
  mutate(agg_klg = ifelse(code_AGG == 1 & code_KLG == 1 & code_SU == 0, 1, 0),
         agg_vlg = ifelse(code_AGG == 1 & code_VLG == 1 & code_SU == 0, 1, 0),
         agg_kg = ifelse(code_AGG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         agg_tg = ifelse(code_AGG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         klg_vlg = ifelse(code_KLG == 1 & code_VLG == 1 & code_SU == 0, 1, 0),
         klg_kg = ifelse(code_KLG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         klg_tg = ifelse(code_KLG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         vlg_kg = ifelse(code_VLG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         vlg_tg = ifelse(code_VLG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         kg_tg = ifelse(code_KG == 1 & code_TG == 1 & code_SU == 0, 1, 0))%>%
  #group_by(Dokumentname)%>%
  summarize(agg_klg = sum(agg_klg),
            agg_vlg = sum(agg_vlg),
            agg_kg = sum(agg_kg),
            agg_tg = sum(agg_tg),
            klg_vlg = sum(klg_vlg),
            klg_kg = sum(klg_kg), 
            klg_tg = sum(klg_tg), 
            vlg_kg = sum(vlg_kg),
            vlg_tg = sum(vlg_tg),
            kg_tg = sum(kg_tg))%>%
  gt(.)
  
```

# nähere Inspektion von agg_klg, agg_vlg, klg_vlg; Rest vernachlässigbar bei 48 Videos

```{r}
data_timesampling%>%
  select(Dokumentname, code_AGG, code_KLG, code_VLG, code_KG, code_TG, code_SU)%>%
  mutate(agg_klg = ifelse(code_AGG == 1 & code_KLG == 1 & code_SU == 0, 1, 0),
         agg_vlg = ifelse(code_AGG == 1 & code_VLG == 1 & code_SU == 0, 1, 0),
         agg_kg = ifelse(code_AGG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         agg_tg = ifelse(code_AGG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         klg_vlg = ifelse(code_KLG == 1 & code_VLG == 1 & code_SU == 0, 1, 0),
         klg_kg = ifelse(code_KLG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         klg_tg = ifelse(code_KLG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         vlg_kg = ifelse(code_VLG == 1 & code_KG == 1 & code_SU == 0, 1, 0),
         vlg_tg = ifelse(code_VLG == 1 & code_TG == 1 & code_SU == 0, 1, 0),
         kg_tg = ifelse(code_KG == 1 & code_TG == 1 & code_SU == 0, 1, 0))%>%
  group_by(Dokumentname)%>%
  summarize(agg_klg = sum(agg_klg),
            agg_vlg = sum(agg_vlg),
            #agg_kg = sum(agg_kg),
            #agg_tg = sum(agg_tg),
            klg_vlg = sum(klg_vlg))%>%
            #klg_kg = sum(klg_kg), 
            #klg_tg = sum(klg_tg), 
            #vlg_kg = sum(vlg_kg),
            #vlg_tg = sum(vlg_tg),
            #kg_tg = sum(kg_tg))%>%
  filter(agg_klg > 3 | agg_vlg > 3 | klg_vlg > 3 )%>%
  gt(.)

```

7 (ca 1/7) Lehrkräfte über benchmark 3 Sekunden, davon 5 Kodierungn \> 7 Sek. Am stärksten: Überschneidungen von klg und agg, macht auch inhaltlich Sinn. Vorerst zurückgestellt.

## Inhaltscodes

Kategorien: RG: Rezeption FA: Fehleranalyse AB: Abgleich Eigeneinschätzung HM: Handlungsmaßnahmen

Überlappungen: RG - FA RG - AB RG - HM FA - AB FA - HM AB - HM

```{r}
data_timesampling%>%
  select(Dokumentname, code_RG, code_FA, code_AB, code_HM, code_SU)%>%
  mutate(rg_fa = ifelse(code_RG == 1 & code_FA== 1 & code_SU == 0, 1, 0),
         rg_ab = ifelse(code_RG == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         rg_hm = ifelse(code_RG == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         fa_ab = ifelse(code_FA == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         fa_hm = ifelse(code_FA == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         ab_hm = ifelse(code_AB == 1 & code_HM== 1 & code_SU == 0, 1, 0))%>%
#group_by(Dokumentname)%>%
  summarize(rg_fa = sum(rg_fa), 
            rg_ab = sum(rg_ab),
            rg_hm = sum(rg_hm),
            fa_ab = sum(fa_ab), 
            fa_hm = sum(fa_hm),
            ab_hm = sum(ab_hm))
```

das scheint alles substanziell.

### sortiert nach Überlappungen:

```{r}
data_timesampling%>%
  select(Dokumentname, code_RG, code_FA, code_AB, code_HM, code_SU)%>%
  mutate(rg_fa = ifelse(code_RG == 1 & code_FA== 1 & code_SU == 0, 1, 0),
         rg_ab = ifelse(code_RG == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         rg_hm = ifelse(code_RG == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         fa_ab = ifelse(code_FA == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         fa_hm = ifelse(code_FA == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         ab_hm = ifelse(code_AB == 1 & code_HM== 1 & code_SU == 0, 1, 0))%>%
group_by(Dokumentname)%>%
  summarize(rg_fa = sum(rg_fa), 
            rg_ab = sum(rg_ab),
            rg_hm = sum(rg_hm),
            fa_ab = sum(fa_ab), 
            fa_hm = sum(fa_hm),
            ab_hm = sum(ab_hm))%>%
  filter(rg_fa > 3 | rg_ab > 3 | rg_hm > 3 | fa_ab > 3 | fa_hm > 3 | ab_hm > 3)%>%
  select(-Dokumentname)%>%
  pivot_longer(everything(), names_to = "codes", values_to = "sek")%>%
  ggplot(aes(codes, sek)) + geom_boxplot() + geom_jitter()
```

```{r}
data_timesampling%>%
  select(Dokumentname, code_RG, code_FA, code_AB, code_HM, code_SU)%>%
  mutate(rg_fa = ifelse(code_RG == 1 & code_FA== 1 & code_SU == 0, 1, 0),
         rg_ab = ifelse(code_RG == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         rg_hm = ifelse(code_RG == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         fa_ab = ifelse(code_FA == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         fa_hm = ifelse(code_FA == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         ab_hm = ifelse(code_AB == 1 & code_HM== 1 & code_SU == 0, 1, 0))%>%
group_by(Dokumentname)%>%
  summarize(rg_fa = sum(rg_fa), 
            rg_ab = sum(rg_ab),
            rg_hm = sum(rg_hm),
            fa_ab = sum(fa_ab), 
            fa_hm = sum(fa_hm),
            ab_hm = sum(ab_hm))%>%
  filter(rg_fa > 3 | rg_ab > 3 | rg_hm > 3 | fa_ab > 3 | fa_hm > 3 | ab_hm > 3)%>%
  select(-Dokumentname)%>%
  pivot_longer(everything(), names_to = "codes", values_to = "sek")%>%
  ggplot(aes(codes, sek)) + geom_boxplot() + geom_jitter() + coord_cartesian(ylim = c(0, 50)) + theme_minimal()
```

### sortiert nach Lehrpersonen

```{r}
data_timesampling%>%
  select(Dokumentname, code_RG, code_FA, code_AB, code_HM, code_SU)%>%
  mutate(rg_fa = ifelse(code_RG == 1 & code_FA== 1 & code_SU == 0, 1, 0),
         rg_ab = ifelse(code_RG == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         rg_hm = ifelse(code_RG == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         fa_ab = ifelse(code_FA == 1 & code_AB== 1 & code_SU == 0, 1, 0), 
         fa_hm = ifelse(code_FA == 1 & code_HM== 1 & code_SU == 0, 1, 0),
         ab_hm = ifelse(code_AB == 1 & code_HM== 1 & code_SU == 0, 1, 0))%>%
group_by(Dokumentname)%>%
  summarize(rg_fa = sum(rg_fa), 
            rg_ab = sum(rg_ab),
            rg_hm = sum(rg_hm),
            fa_ab = sum(fa_ab), 
            fa_hm = sum(fa_hm),
            ab_hm = sum(ab_hm))%>%
  filter(rg_fa > 3 | rg_ab > 3 | rg_hm > 3 | fa_ab > 3 | fa_hm > 3 | ab_hm > 3)%>%
  pivot_longer(!Dokumentname, names_to = "codes", values_to = "sek")%>%
  ggplot(aes(sek, Dokumentname, color = codes)) + geom_jitter() + theme_minimal() + coord_cartesian(xlim = c(0, 100))
```

nicht ganz so informativ wie gedacht ...

# 3. Exploring HeuristicsMiner bzw. bupaverse

Beispieldatensätze

```{r}

# testcode
data(patients)

# Dependency graph / matrix
dependency_matrix(patients)
# Causal graph / Heuristics net
causal_net(patients)

View(patients)

```

auch in den patients daten gibt es "Lücken", d.h. die events gehen nicht direkt ineinander über.

## 3.1 event logs 1. Versuch, alle events am selben Tag

### wrangling

```{r}

eventlog_test <- konsens_event_ta_o%>% 
  rename(complete = end)%>%
  convert_timestamps(columns = c("start", "complete"), format = ymd_hms)%>%
  mutate(test = c(1))%>% # man braucht das auf jeden Fall, ohne resource id kann man das objekt nicht generieren, daher dummy var 
   activitylog(case_id = "Dokumentname",
                activity_id = "code",
                timestamps = c("start", "complete"), 
               resource_id = "test")%>% #d
  to_eventlog() # von eventlog zu activitylog geht recht schnell, es gibt auch die inverse Funktion 


```

-   resource id?
-   case_id = Grafiken und "Zuordnung" insgesamt?

Causal net und petri net

```{r}

# Dependency graph / matrix
dependency_matrix(eventlog_test)

# Causal graph / Heuristics net
causal_net(eventlog_test)
 
causal_net(eventlog_test, threshold = .7)

causal_net(eventlog_test, threshold = .5)


# petri net 
# Convert to Petri net
cn <- causal_net(eventlog_test, threshold = .7)
pn <- as.petrinet(cn)
render_PN(pn)

```

### Visual inspections

```{r}

eventlog_test %>% activity_presence() %>%
    plot


eventlog_test %>% 
   trace_explorer(n_traces = 10,
                  type = c("frequent"),
                   scale_fill = ggplot2::scale_fill_discrete)

eventlog_test %>% 
   trace_explorer(n_traces = 10,
                  type = c("infrequent"),
                   scale_fill = ggplot2::scale_fill_discrete) # für mich kein unterschied sichtbar zwischen frequent und infrequent 


eventlog_test %>%
    ps_detailed()

```

## 3.2 event logs 2. Versuch: creating event logs with individual days per teacher

### wrangling

```{r}
individual_dates <- konsens_event_ta_o%>%
  select(Dokumentname)%>%
  unique()%>%
  mutate(day = seq(ymd('2000-01-01'), ymd('2000-02-17'), by='1 day')) # 48 Tage in Folge für 48 Lehrkräfte ab dem 01.01.2000
  
  
data_konsens_event_corrected_ind_dates <- read_excel("data/MAXQDA 2018 Codings 3_fertig_Runde1.xlsx")%>%
  select(Dokumentgruppe, Dokumentname, Anfang, Ende, Code)%>%
    filter(Dokumentgruppe == "Konsens")%>%
  left_join(., individual_dates, by = "Dokumentname")%>%
    mutate(Anfang_2 = str_sub(Anfang, -1),
         Ende_2 = str_sub(Ende, -1))%>%
  mutate(start_1 = ymd_hms(ymd(day) + hms(Anfang)), 
         end_1 = ymd_hms(ymd(day) + hms(Ende)), 
         turnID = 1:n(),
        code = Code)%>%
  mutate(start = case_when(Anfang_2 > 4 ~ (ymd_hms(start_1) + seconds(1)), 
                           TRUE~ ymd_hms(start_1)))%>%
  mutate(end = case_when(Ende_2 > 4 ~ (ymd_hms(end_1) + seconds(1)), 
                         TRUE ~ ymd_hms(end_1)))%>%
  select(Dokumentname, Dokumentgruppe, code, turnID, start, end)
  

  
  max_TA_O_ind_dates <- data_konsens_event_corrected_ind_dates%>% 
  select(-Dokumentgruppe)%>%
  filter(code %in% c("TA_O"))%>%
  group_by(Dokumentname)%>%
  summarize(max_TA_O = max(ymd_hms(end)))

konsens_event_ta_o_ind_dates <- data_konsens_event_corrected_ind_dates%>% 
  select(-Dokumentgruppe)%>%
  filter(code %in% c("RG", "AB", "FA", "HM"))%>%
  left_join(., max_TA_O_ind_dates, by = "Dokumentname")%>%
  filter(end <= max_TA_O)%>%
  select(-max_TA_O)

eventlog_test_ind_dates <- konsens_event_ta_o_ind_dates%>% 
  rename(complete = end)%>%
  convert_timestamps(columns = c("start", "complete"), format = ymd_hms)%>%
  mutate(test = as.character("a"))%>% # man braucht das auf jeden Fall, ohne resource id kann man das objekt nicht generieren, daher dummy var 
   activitylog(case_id = "Dokumentname",
                activity_id = "code",
                timestamps = c("start", "complete"), 
               resource_id = "test")%>% #d
  to_eventlog() # von eventlog zu activitylog geht recht schnell, es gibt auch die inverse Funktion 


```

### Visual inspections

```{r}
# Dependency graph / matrix
dependency_matrix(
  eventlog_test_ind_dates |> 
    group_by(),
  threshold = 0)

# Causal graph / Heuristics net
causal_net(eventlog_test_ind_dates)
 
causal_net(eventlog_test_ind_dates, threshold = .7)

causal_net(eventlog_test_ind_dates, threshold = .5)


# petri net 
# Convert to Petri net
cn <- causal_net(eventlog_test_ind_dates, threshold = .7)
pn <- as.petrinet(cn)
render_PN(pn)

```

```{r}

eventlog_test_ind_dates %>% 
  activity_presence() %>%
    plot


eventlog_test_ind_dates %>% 
   trace_explorer(n_traces = 10,
                  type = c("frequent"),
                   scale_fill = ggplot2::scale_fill_discrete)

eventlog_test_ind_dates %>% 
   trace_explorer(n_traces = 10,
                  type = c("infrequent"),
                   scale_fill = ggplot2::scale_fill_discrete) # für mich kein unterschied sichtbar zwischen frequent und infrequent 


eventlog_test_ind_dates %>%
    ps_detailed()

```
